{"hostfile": "/mock_path", "train_batch_size": 256, "train_micro_batch_size_per_gpu": 32, "optimizer": {"type": "Adam", "params": {"lr": 0.0006, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"enabled": true, "type": "bfloat16", "loss_scale": 0, "loss_scale_window": 1000, "hysteresis": 2, "min_loss_scale": 1}, "gradient_clipping": 1.0, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true}, "steps_per_print": 200, "precision": "bfloat16", "num_layers": 12, "hidden_size": 768, "num_attention_heads": 12, "seq_length": 2048, "max_position_embeddings": 2048, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global"], "sparsity_config": {}, "init_method": "small_init", "output_layer_init_method": "wang_init", "output_layer_parallelism": "column", "lr_decay_style": "cosine", "lr_decay_iters": 5000, "min_lr": 6e-05, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.0006, "train_data_paths": ["/work/data/mpoli_data/pile/pile_text_document"], "test_data_paths": ["/work/data/mpoli_data/pile_test/pile_test_text_document"], "valid_data_paths": ["/work/data/mpoli_data/pile_validation/pile_validation_text_document"], "train_data_weights": [1.0], "valid_data_weights": [1.0], "test_data_weights": [1.0], "data_impl": "mmap", "save": "checkpoints", "config_files": {"the_pile.yml": "{\n  \"train-data-paths\": [\"/work/data/mpoli_data/pile/pile_text_document\"],\n  \"test-data-paths\": [\"/work/data/mpoli_data/pile_test/pile_test_text_document\"],\n  \"valid-data-paths\": [\"/work/data/mpoli_data/pile_validation/pile_validation_text_document\"],\n  \"test-data-weights\": [1.],\n  \"valid-data-weights\": [1.],\n  \"vocab-file\": \"/work/data/mpoli_data/gpt2-vocab.json\",\n  \"merge-file\": \"/work/data/mpoli_data/gpt2-merges.txt\",\n  \"save\": \"checkpoints\",\n  \"load\": \"checkpoints\",\n  \"checkpoint_validation_with_forward_pass\": False,\n  \"use_wandb\": True,\n  \"wandb_host\": \"https://api.wandb.ai\",\n  \"wandb_project\": \"hyena-neox\"\n}\n", "125M_GPT_nomlp_15B.yml": "{\n   \"pipe-parallel-size\": 1,\n   \"model-parallel-size\": 1,\n\n   ############### model settings\n   \"num-layers\": 12,\n   \"hidden-size\": 768,\n   \"num-attention-heads\": 12,\n   \"seq-length\": 2048,\n   \"max-position-embeddings\": 2048,\n   \"norm\": \"layernorm\",\n   \"pos-emb\": \"rotary\",\n   \"no-weight-tying\": true,\n   \"gpt_j_residual\": false,\n   \"output_layer_parallelism\": \"column\",\n\n   ############## misc optims\n   \"scaled-upper-triang-masked-softmax-fusion\": false,\n   \"bias-gelu-fusion\": false,\n\n   ############### init\n   \"init_method\": \"small_init\",\n   \"output_layer_init_method\": \"wang_init\",\n\n   ############## optimizer\n   \"optimizer\": {\n     \"type\": \"Adam\",\n     \"params\": {\n       \"lr\": 0.0006,\n       \"betas\": [0.9, 0.95],\n       \"eps\": 1.0e-8,\n     }\n   },\n   \"min_lr\": 0.00006,\n\n   ############### sys optims\n   \"zero_optimization\": {\n    \"stage\": 1,\n    \"allgather_partitions\": True,\n    \"allgather_bucket_size\": 500000000,\n    \"overlap_comm\": True,\n    \"reduce_scatter\": True,\n    \"reduce_bucket_size\": 500000000,\n    \"contiguous_gradients\": True,\n  },\n\n   ################# batch sizes\n   \"train_micro_batch_size_per_gpu\": 32,\n   \"data-impl\": \"mmap\",\n   \"train_batch_size\": 256,\n\n   ################# act checkpointing\n   \"checkpoint-activations\": true,\n   \"checkpoint-num-layers\": 1,\n   \"partition-activations\": true,\n   \"synchronize-each-layer\": true,\n\n   ################# regularization\n   \"gradient_clipping\": 1.0,\n   \"weight-decay\": 0.1,\n   \"hidden-dropout\": 0.0,\n   \"attention-dropout\": 0.0,\n\n   ################ precision\n   \"fp16\": {\n     \"enabled\": true,\n     \"type\": \"bfloat16\", \n     \"loss_scale\": 0,\n     \"loss_scale_window\": 1000,\n     \"hysteresis\": 2,\n     \"min_loss_scale\": 1\n   },\n\n   ################# training\n   \"train-iters\": 5000,\n   \"lr-decay-iters\": 5000,\n   \"distributed-backend\": \"nccl\",\n   \"lr-decay-style\": \"cosine\",\n   \"warmup\": 0.01,\n   \"checkpoint-factor\": 10000,\n   \"eval-interval\": 1000,\n   \"eval-iters\": 10,\n\n   ################ logging\n   \"log-interval\": 200,\n   \"steps_per_print\": 200,\n   \"keep-last-n-checkpoints\": 4,\n   \"wall_clock_breakdown\": false,\n\n  ################# networking\n  \"hostfile\": \"/mock_path\"\n}\n\n"}, "load": "checkpoints", "checkpoint_factor": 10000, "batch_size": 32, "train_iters": 5000, "eval_iters": 10, "keep_last_n_checkpoints": 4, "vocab_file": "/work/data/mpoli_data/gpt2-vocab.json", "merge_file": "/work/data/mpoli_data/gpt2-merges.txt", "attention_dropout": 0.0, "hidden_dropout": 0.0, "weight_decay": 0.1, "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "gas": 1, "clip_grad": 1.0, "dynamic_loss_scale": true, "pipe_parallel_size": 1, "world_size": 1, "use_wandb": true, "wandb_group": "vu80irzq", "wandb_project": "hyena-neox", "log_interval": 200, "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "user_script": "train.py", "save_iters": [], "global_num_gpus": 8}